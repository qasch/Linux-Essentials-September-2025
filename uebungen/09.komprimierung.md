# Übungen zur Kompression unter Linux

## Vorbereitung

Folgende Kommandos werden für die Übungen benötigt:
```bash
dd
tar
gzip
bzip2
xz
time
```
1. Erstellt ein Verzeichnis `uebungen-kompression` für die Übungen und wechselt hinein.
2. Erstellt für die Übungen drei Testdateien mit dem Kommando `dd` wie folgt:
```bash
dd if=/dev/zero of=bigfile_gzip.data bs=1M count=1024
dd if=/dev/zero of=bigfile_bzip2.data bs=1M count=1024
dd if=/dev/zero of=bigfile_xz.data bs=1M count=1024
```
**Erklärung:** `dd` liest hier von der Gerätedatei `/dev/zero/` 1024 Blöcke mit Nullbytes zu je 1MB ein und erstellt daraus die Datei `bigfile_gzip.data` im aktuellen Verzeichnis. Das Resultat ist eine 1 GB grosse Datei.

>[!NOTE]
> Wenn euch die Komprimierung und Dekomprimierung mit den drei 1GB grossen Dateien zu lange dauern sollte, könnt ihr auch weniger grosse Dateien erstellen. Dazu übergebt ihr mit `count` einfach einen anderen Wert, z.B. `512`.
```bash
dd if=/dev/zero of=mediumfile_gzip.data bs=1M count=512
```
### 1. Komprimieren einzelner Dateien
Verwendet nacheinander `gzip`, `bzip2` und `xz`, um eine einzelne Datei direkt zu komprimieren. 

1. Komprimiert zuerst `bigfile_gzip.data` mit `gzip`, dann `bigfile_bzip2.data` mit `bzip2` und `bigfile_xz.data` mit `xz`. Was fällt euch auf? Was ist mit den Originaldateien passiert? Schaut vielleicht in den Manpages mal nach der Option `-k`.
2. Messt mit dem Kommando `time` wie lange die einzelnen Komprimierungsalgorithmen zur Komprimierung brauchen. 

>[!WARNING]
> Falls ihr die Dateien bereits einmal komprimiert und dekomprimiert haben solltet, löscht diese wieder und erstellt für die Messung der Zeiten und Größen **neue** Dateien. Ansonsten werden die Ergebnisse verfälscht, da in unserem speziellen Fall mit den Dateien mit Nullbytes diese nicht wieder in den Originalzustand versetzt werden.
```bash
time <kommando>
time gzip bigfile_gzip.data
```
3. Wie gross sind die resultierenden Dateien? 
```bash
ls -lh <datei>
du -h <datei>
```
4. Was schließt ihr daraus bezüglich der einzelnen Algorithmen?
5. Ist `xz` am langsamsten, also am "schlechtesten"? Es ist doch der neueste der drei Komprimierungsalgorithmen. Oder solltet ihr euch vielleicht auch noch das Verhalten beim **Dekomprimieren** anschauen? Testet doch auch das mal mit dem Kommando `time`. Dekomprimiert also alle drei Dateien wieder und messt auch hier die Dauer mit `time`.
6. Mit welchem Hintergrund wurde also `xz` entwickelt? Es ist wie gesagt der neuste der drei Komprimierungsalgorithmen.

>[!NOTE]
> Macht euch vielleicht Screenshots der einzelnen Ergebnisse oder arbeitet mit zwei Terminalfenstern, so dass ihr den Überblick behaltet.

### 2. Erstellen eines Tar-Archivs ohne Kompression
Erstellt nun ein Tar-Archiv `backup.tar`, das alle Dateien aus dem aktuellen Verzeichnis enthält, also `bigfile_gzip.data`, `bigfile_bzip2.data` und `bigfile_xz.data`.

Achtet darauf, dass sich ausschließlich diese drei Dateien in eurem Verzeichnis befinden.

### 3. Erstellen eines komprimierten Tar-Archivs
Erstellt nun ein gzip-komprimiertes Archiv `backup.tar.gz` aus dem `tar`-Archiv und überprüft die Dateigröße. Dazu könnt ihr das erstellte Archiv einfach mit `gzip` komprimieren. Das sind aber zwei Schritte, die nacheinander durchgeführt werden müssen. Ist doch mühsam. Vielleicht geht das ja auch in einem Schritt? Kann `tar` das etwa direkt? 

1. Sucht doch in der Manpage von `tar` einmal nach dem Stichwort `Compression` bzw. nach `Kompression` falls eure Manpage auf Deutsch ist und erstellt ein komprimiertes Archiv mit `tar` in einem Rutsch.
2. Dekomprimiert das komprimierte Archiv nun einmal mit `tar`.
3. Führt die beiden Schritte Komprimierung und Dekomprimierung einmal für alle drei Komprimierungsalgorithmen `gzip`, `bzip2` und `xz` direkt mit `tar` durch. Notiert euch die jeweiligen Optionen. 
4. Wie könnt ihr euch diese merken? Gibt es vielleicht eine Art Eselsbrücke dafür?

### 4. Dekomprimieren der komprimierten Archive mit `tar`
Muss bei der Dekomprimierung mit `tar` zwingend der jeweilige Algorithmus als Option angegeben werden? Oder gibt es auch andere Möglichkeiten?

### 5. Falsche Dateiendung
1. Was passiert, wenn ihr eine Datei z.B. mit `bzip2` komprimiert, ihr aber aus Versehen die Dateiendung `.gz` verpasst? Ist sie dann `gzip` komprimiert? Überprüft die Datei mit dem Kommando `file`. 
2. Was passiert, wenn ihr sie nun mit `tar -xf` (ohne Angabe des Komprimierungsalgorithmus) entpacken möchtet?

### 6. Ein Verzeichnis direkt mit gzip, bzip2 oder xz komprimieren
Versucht, ein Verzeichnis direkt mit `gzip`, `bzip2` oder `xz` zu komprimieren, ohne `tar`. Funktioniert das? Warum bzw. warum nicht?

### 7. Komprimierung mit zip
Erstellt ein ZIP-Archiv `backup.zip`, das alle Dateien eines Verzeichnisses enthält (also z.B. alle `bigfile` Dateien). 

Wie unterscheidet sich das Verhalten von `zip` im Vergleich zu `tar` mit `gzip`, `bzip2` und `xz`? Ihr könnt (falls ihr wollt) auch hier einmal die Kompressionsgeschwindigkeit und resultierende Dateigrösse vergleichen.

> [!NOTE]
> `zip` ist kein natives Linux Tool, daher ist es standardmässig nicht installiert. Nachholen könnt ihr das mit `apt install zip` als `root`.

## Weiterführende Übungen

### 8. Doppelte Kompression
1. Was passiert, wenn ihr eine Datei mit `gzip` komprimiert und anschließend die komprimierte Datei erneut mit `gzip` komprimiert?
2. Was passiert, wenn ihr eine mit `gzip` komprimierte Datei zusätzlich mit z.B. `xz` komprimiert?

### 9. Komprimierungsalgorithmen 
Erstellt eine weitere Testdatei `bigfile_random.data` mit folgendem Kommando:
```bash
dd if=/dev/random of=bigfile_random.data bs=1M count=1024
```
Ihr verwendet hier die Gerätedatei `/dev/random`. Diese produziert keine Nullbytes, sondern "Pseudozufall".

Komprimiert diese Datei nun mit einem Algorithmus eurer Wahl oder auch mit allen drei. Was fällt euch auf? Was könnt ihr daraus über das generelle Vorgehen bzw. die Arbeitsweise eines Komprimierungsalgorithmus schliessen?
